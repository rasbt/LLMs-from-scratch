{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c",
      "metadata": {
        "id": "e1b280ab-b61f-4d1a-bf7e-44e5f9ed3a5c"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "Supplementary code for the <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> book by <a href=\"https://sebastianraschka.com\">Sebastian Raschka</a><br>\n",
        "<br>Code repository: <a href=\"https://github.com/rasbt/LLMs-from-scratch\">https://github.com/rasbt/LLMs-from-scratch</a>\n",
        "</font>\n",
        "</td>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efde77f2-6af3-4781-8597-89ecd3f41a52",
      "metadata": {
        "id": "efde77f2-6af3-4781-8597-89ecd3f41a52"
      },
      "source": [
        "# Gemma 3 270M From Scratch (A Standalone Notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d",
      "metadata": {
        "id": "55cdef4d-de59-4a65-89f9-fa2a8ef3471d"
      },
      "source": [
        "- This notebook is purposefully minimal and focuses on the code to re-implement Gemma 3 270M in pure PyTorch without relying on other external LLM libraries\n",
        "- For more information, see the official [Gemma 3 270M model card](https://huggingface.co/google/gemma-3-270m)\n",
        "\n",
        "- Below is a side-by-side comparison with Qwen3 0.6B as a reference model; if you are interested in the Qwen3 0.6B standalone notebook, you can find it [here](../11_qwen3)\n",
        "<br>\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gemma3/gemma3-vs-qwen3.webp?1\">\n",
        "  \n",
        "  \n",
        "- About the code:\n",
        "  - all code is my own code, mapping the Gemma 3 architecture onto the model code implemented in my [Build A Large Language Model (From Scratch)](http://mng.bz/orYv) book; the code is released under a permissive open-source Apache 2.0 license (see [LICENSE.txt](https://github.com/rasbt/LLMs-from-scratch/blob/main/LICENSE.txt))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c201adb-747e-437b-9a62-442802941e01",
      "metadata": {
        "id": "7c201adb-747e-437b-9a62-442802941e01"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df",
      "metadata": {
        "id": "dd1b65a8-4301-444a-bd7c-a6f2bd1df9df"
      },
      "outputs": [],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "# 定义需要检查版本的包列表\n",
        "pkgs = [\n",
        "    \"huggingface_hub\",  # 用于下载预训练权重\n",
        "    \"tokenizers\",       # 用于实现分词器\n",
        "    \"torch\",            # 用于实现模型\n",
        "]\n",
        "# 遍历包列表并打印每个包的版本\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07e96fbb-8e16-4f6d-835f-c6159321280b",
      "metadata": {
        "id": "07e96fbb-8e16-4f6d-835f-c6159321280b"
      },
      "source": [
        "- This notebook supports both the base model and the instructmodel; which model to use can be controlled via the following flag:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70a90338-624a-4706-aa55-6b4358070194",
      "metadata": {
        "id": "70a90338-624a-4706-aa55-6b4358070194"
      },
      "outputs": [],
      "source": [
        "# 定义一个布尔标志，用于控制是否使用指令模型\n",
        "USE_INSTRUCT_MODEL = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "653410a6-dd2b-4eb2-a722-23d9782e726d",
      "metadata": {
        "id": "653410a6-dd2b-4eb2-a722-23d9782e726d"
      },
      "source": [
        "&nbsp;\n",
        "# 1. Architecture code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82076c21-9331-4dcd-b017-42b046cf1a60",
      "metadata": {
        "id": "82076c21-9331-4dcd-b017-42b046cf1a60"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 定义前馈网络类\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # 定义第一个线性层，输入维度 emb_dim，输出维度 hidden_dim\n",
        "        self.fc1 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        # 定义第二个线性层，输入维度 emb_dim，输出维度 hidden_dim\n",
        "        self.fc2 = nn.Linear(cfg[\"emb_dim\"], cfg[\"hidden_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "        # 定义第三个线性层，输入维度 hidden_dim，输出维度 emb_dim\n",
        "        self.fc3 = nn.Linear(cfg[\"hidden_dim\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"], bias=False)\n",
        "\n",
        "    # 定义前向传播\n",
        "    def forward(self, x):\n",
        "        # 应用第一个线性层\n",
        "        x_fc1 = self.fc1(x)\n",
        "        # 应用第二个线性层\n",
        "        x_fc2 = self.fc2(x)\n",
        "        # 应用 GELU 激活函数并与 x_fc2 相乘\n",
        "        x = nn.functional.gelu(x_fc1, approximate=\"tanh\") * x_fc2\n",
        "        # 应用第三个线性层\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56715760-37e1-433e-89da-04864c139a9e",
      "metadata": {
        "id": "56715760-37e1-433e-89da-04864c139a9e"
      },
      "outputs": [],
      "source": [
        "# 定义 RMSNorm 类\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, emb_dim, eps=1e-6, bias=False):\n",
        "        super().__init__()\n",
        "        self.eps = eps # 定义 epsilon，用于防止除零\n",
        "        # Gemma3 存储零中心权重并在前向传播中使用 (1 + weight)\n",
        "        self.scale = nn.Parameter(torch.zeros(emb_dim)) # 定义可学习的缩放参数\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim)) if bias else None # 定义可选的可学习的偏移参数\n",
        "\n",
        "    # 定义前向传播\n",
        "    def forward(self, x):\n",
        "        # 匹配 HF Gemma3：在 float32 中计算范数，然后按 (1 + w) 缩放\n",
        "        input_dtype = x.dtype # 保存输入数据的原始数据类型\n",
        "        x_f = x.float() # 将输入数据转换为 float32\n",
        "        var = x_f.pow(2).mean(dim=-1, keepdim=True) # 计算输入数据的方差\n",
        "        x_norm = x_f * torch.rsqrt(var + self.eps) # 应用 RMS 归一化\n",
        "        out = x_norm * (1.0 + self.scale.float()) # 应用缩放\n",
        "\n",
        "        # 如果存在偏移参数，则应用偏移\n",
        "        if self.shift is not None:\n",
        "            out = out + self.shift.float()\n",
        "\n",
        "        return out.to(input_dtype) # 将输出数据转换回原始数据类型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9a346f-5826-4083-9162-abd56afc03f0",
      "metadata": {
        "id": "4b9a346f-5826-4083-9162-abd56afc03f0"
      },
      "outputs": [],
      "source": [
        "# 计算 RoPE (Rotary Positional Embedding) 参数\n",
        "def compute_rope_params(head_dim, theta_base=10_000, context_length=4096, dtype=torch.float32):\n",
        "    assert head_dim % 2 == 0, \"Embedding dimension must be even\" # 确保 embedding 维度是偶数\n",
        "\n",
        "    # 计算逆频率\n",
        "    # 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "    inv_freq = 1.0 / (theta_base ** (torch.arange(0, head_dim, 2, dtype=dtype)[: (head_dim // 2)].float() / head_dim))\n",
        "\n",
        "    # 生成位置索引\n",
        "    positions = torch.arange(context_length, dtype=dtype)\n",
        "\n",
        "    # 计算角度\n",
        "    # positions[:, None] 的形状是 (context_length, 1)\n",
        "    # inv_freq[None, :] 的形状是 (1, head_dim // 2)\n",
        "    # 相乘得到形状为 (context_length, head_dim // 2) 的角度\n",
        "    angles = positions[:, None] * inv_freq[None, :]\n",
        "\n",
        "    # 扩展角度以匹配 head_dim\n",
        "    # 将角度自身连接起来，得到形状为 (context_length, head_dim)\n",
        "    angles = torch.cat([angles, angles], dim=1)\n",
        "\n",
        "    # 预计算 sine 和 cosine\n",
        "    cos = torch.cos(angles)\n",
        "    sin = torch.sin(angles)\n",
        "\n",
        "    return cos, sin # 返回 cosine 和 sine\n",
        "\n",
        "# 应用 RoPE\n",
        "def apply_rope(x, cos, sin):\n",
        "    # x: (batch_size, num_heads, seq_len, head_dim)\n",
        "    batch_size, num_heads, seq_len, head_dim = x.shape\n",
        "    assert head_dim % 2 == 0, \"Head dimension must be even\" # 确保 head 维度是偶数\n",
        "\n",
        "    # 将 x 分割成前半部分和后半部分\n",
        "    x1 = x[..., : head_dim // 2]  # 前半部分\n",
        "    x2 = x[..., head_dim // 2 :]  # 后半部分\n",
        "\n",
        "    # 调整 sin 和 cos 的形状以进行广播\n",
        "    # unsqueeze(0) 两次，添加批次维度和头维度，形状变为 (1, 1, seq_len, head_dim)\n",
        "    cos = cos[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "    sin = sin[:seq_len, :].unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "    # 应用旋转变换\n",
        "    # 将 x2 的符号取反并与 x1 连接，得到旋转后的向量\n",
        "    rotated = torch.cat((-x2, x1), dim=-1)\n",
        "    # 应用 RoPE 公式: x * cos + rotated * sin\n",
        "    x_rotated = (x * cos) + (rotated * sin)\n",
        "\n",
        "    # 应用 cos 和 sin 旋转后可以使用较低精度\n",
        "    return x_rotated.to(dtype=x.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb",
      "metadata": {
        "id": "e8169ab5-f976-4222-a2e1-eb1cabf267cb"
      },
      "outputs": [],
      "source": [
        "# 定义分组查询注意力机制 (Grouped Query Attention)\n",
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self, d_in, num_heads, num_kv_groups, head_dim=None, qk_norm=False,\n",
        "        query_pre_attn_scalar=None, dtype=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # 确保 num_heads 能被 num_kv_groups 整除\n",
        "        assert num_heads % num_kv_groups == 0, \"num_heads must be divisible by num_kv_groups\"\n",
        "\n",
        "        self.num_heads = num_heads # 查询头的数量\n",
        "        self.num_kv_groups = num_kv_groups # KV 组的数量\n",
        "        self.group_size = num_heads // num_kv_groups # 每个 KV 组对应的查询头数量\n",
        "\n",
        "        # 如果 head_dim 没有设置，则根据 d_in 和 num_heads 计算\n",
        "        if head_dim is None:\n",
        "            assert d_in % num_heads == 0, \"`d_in` must be divisible by `num_heads` if `head_dim` is not set\"\n",
        "            head_dim = d_in // num_heads\n",
        "\n",
        "        self.head_dim = head_dim # 每个头的维度\n",
        "        self.d_out = num_heads * head_dim # 输出维度\n",
        "\n",
        "        # 定义查询、键、值的线性投影层\n",
        "        self.W_query = nn.Linear(d_in, self.d_out, bias=False, dtype=dtype) # 查询投影\n",
        "        self.W_key = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype) # 键投影\n",
        "        self.W_value = nn.Linear(d_in, num_kv_groups * head_dim, bias=False, dtype=dtype) # 值投影\n",
        "\n",
        "        # 定义输出投影层\n",
        "        self.out_proj = nn.Linear(self.d_out, d_in, bias=False, dtype=dtype)\n",
        "\n",
        "        # 如果需要进行 QK 归一化，则定义 Q 归一化和 K 归一化层\n",
        "        if qk_norm:\n",
        "            self.q_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "            self.k_norm = RMSNorm(head_dim, eps=1e-6)\n",
        "        else:\n",
        "            self.q_norm = self.k_norm = None\n",
        "\n",
        "        # 计算缩放因子\n",
        "        if query_pre_attn_scalar is not None:\n",
        "            self.scaling = (query_pre_attn_scalar) ** -0.5\n",
        "        else:\n",
        "            self.scaling = (head_dim) ** -0.5\n",
        "\n",
        "\n",
        "    # 定义前向传播\n",
        "    def forward(self, x, mask, cos, sin):\n",
        "        b, num_tokens, _ = x.shape # 获取批次大小和序列长度\n",
        "\n",
        "        # 应用投影层\n",
        "        queries = self.W_query(x)  # (b, num_tokens, num_heads * head_dim)\n",
        "        keys = self.W_key(x)       # (b, num_tokens, num_kv_groups * head_dim)\n",
        "        values = self.W_value(x)   # (b, num_tokens, num_kv_groups * head_dim)\n",
        "\n",
        "        # 重塑形状以进行多头注意力计算\n",
        "        # 将 num_heads * head_dim 拆分成 num_heads 和 head_dim\n",
        "        # 转置维度 1 和 2，使头的维度在前\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        # 将 num_kv_groups * head_dim 拆分成 num_kv_groups 和 head_dim\n",
        "        # 转置维度 1 和 2，使 KV 组的维度在前\n",
        "        keys = keys.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "        values = values.view(b, num_tokens, self.num_kv_groups, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # 可选的归一化\n",
        "        if self.q_norm:\n",
        "            queries = self.q_norm(queries)\n",
        "        if self.k_norm:\n",
        "            keys = self.k_norm(keys)\n",
        "\n",
        "        # 应用 RoPE\n",
        "        queries = apply_rope(queries, cos, sin)\n",
        "        keys = apply_rope(keys, cos, sin)\n",
        "\n",
        "        # 扩展 K 和 V 以匹配查询头的数量\n",
        "        # 使用 repeat_interleave 将每个 KV 组重复 group_size 次\n",
        "        keys = keys.repeat_interleave(self.group_size, dim=1)\n",
        "        values = values.repeat_interleave(self.group_size, dim=1)\n",
        "\n",
        "        # 缩放查询\n",
        "        queries = queries * self.scaling\n",
        "\n",
        "        # 计算注意力分数\n",
        "        # 查询与键的转置相乘\n",
        "        attn_scores = queries @ keys.transpose(2, 3)\n",
        "        # 应用掩码，将需要屏蔽的位置设置为负无穷\n",
        "        attn_scores = attn_scores.masked_fill(mask, -torch.inf)\n",
        "        # 应用 softmax 计算注意力权重\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # 计算上下文向量\n",
        "        # 注意力权重与值相乘\n",
        "        context = (attn_weights @ values).transpose(1, 2).reshape(b, num_tokens, self.d_out)\n",
        "        # 应用输出投影层\n",
        "        return self.out_proj(context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9",
      "metadata": {
        "id": "457cb2f8-50c1-4045-8a74-f181bfb5fea9"
      },
      "outputs": [],
      "source": [
        "# 定义 Transformer 块\n",
        "class TransformerBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, cfg: dict, attn_type: str):\n",
        "        super().__init__()\n",
        "        self.attn_type = attn_type # 定义注意力类型 (滑动窗口注意力或全局注意力)\n",
        "\n",
        "        # 定义分组查询注意力层\n",
        "        self.att = GroupedQueryAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            num_kv_groups=cfg[\"n_kv_groups\"],\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            qk_norm=cfg[\"qk_norm\"],\n",
        "            query_pre_attn_scalar=cfg[\"query_pre_attn_scalar\"],\n",
        "            dtype=cfg[\"dtype\"],\n",
        "        )\n",
        "        # 定义前馈网络层\n",
        "        self.ff = FeedForward(cfg)\n",
        "        # 定义各种 LayerNorm 层\n",
        "        self.input_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6) # 输入 LayerNorm\n",
        "        self.post_attention_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6) # 注意力后 LayerNorm\n",
        "        self.pre_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6) # 前馈网络前 LayerNorm\n",
        "        self.post_feedforward_layernorm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6) # 前馈网络后 LayerNorm\n",
        "\n",
        "    # 定义前向传播\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        mask_global, # 全局注意力掩码\n",
        "        mask_local, # 滑动窗口注意力掩码\n",
        "        cos_global, # 全局 RoPE cos\n",
        "        sin_global, # 全局 RoPE sin\n",
        "        cos_local, # 滑动窗口 RoPE cos\n",
        "        sin_local, # 滑动窗口 RoPE sin\n",
        "    ):\n",
        "        # 注意力块的 shortcut 连接\n",
        "        shortcut = x\n",
        "        # 对输入应用 LayerNorm\n",
        "        x = self.input_layernorm(x)\n",
        "\n",
        "        # 根据注意力类型选择对应的掩码和 RoPE 参数\n",
        "        if self.attn_type == \"sliding_attention\":\n",
        "            attn_mask = mask_local\n",
        "            cos = cos_local\n",
        "            sin = sin_local\n",
        "        else:\n",
        "            attn_mask = mask_global\n",
        "            cos = cos_global\n",
        "            sin = sin_global\n",
        "\n",
        "        # 应用注意力机制\n",
        "        x_attn = self.att(x, attn_mask, cos, sin)\n",
        "        # 对注意力输出应用 LayerNorm\n",
        "        x_attn = self.post_attention_layernorm(x_attn)\n",
        "        # 将注意力输出加到 shortcut 连接上\n",
        "        x = shortcut + x_attn\n",
        "\n",
        "        # 前馈块的 shortcut 连接\n",
        "        shortcut = x\n",
        "        # 对输入应用前馈网络前的 LayerNorm\n",
        "        x_ffn = self.pre_feedforward_layernorm(x)\n",
        "        # 应用前馈网络\n",
        "        x_ffn = self.ff(x_ffn)\n",
        "        # 对前馈网络输出应用后 LayerNorm\n",
        "        x_ffn = self.post_feedforward_layernorm(x_ffn)\n",
        "        # 将前馈网络输出加到 shortcut 连接上\n",
        "        x = shortcut + x_ffn\n",
        "        return x # 返回 Transformer 块的输出"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4",
      "metadata": {
        "id": "e88de3e3-9f07-42cc-816b-28dbd46e96c4"
      },
      "outputs": [],
      "source": [
        "# 定义 Gemma3 模型\n",
        "class Gemma3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        # 确保 layer_types 配置存在且长度等于层数\n",
        "        assert cfg[\"layer_types\"] is not None and len(cfg[\"layer_types\"]) == cfg[\"n_layers\"]\n",
        "\n",
        "        # 主要模型参数\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"], dtype=cfg[\"dtype\"]) # token embedding 层\n",
        "\n",
        "        # Transformer 块列表，根据 layer_types 配置构建不同类型的注意力块\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(cfg, attn_type)for attn_type in cfg[\"layer_types\"]\n",
        "        ])\n",
        "\n",
        "        self.final_norm = RMSNorm(cfg[\"emb_dim\"], eps=1e-6) # 最终 LayerNorm\n",
        "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False, dtype=cfg[\"dtype\"]) # 输出头 (用于预测下一个 token)\n",
        "        self.cfg = cfg # 保存配置字典\n",
        "\n",
        "        # 可重复使用的工具\n",
        "        # 计算局部 RoPE 参数 (用于滑动窗口注意力)\n",
        "        cos_local, sin_local = compute_rope_params(\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            theta_base=cfg[\"rope_local_base\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "        # 计算全局 RoPE 参数 (用于全局注意力)\n",
        "        cos_global, sin_global = compute_rope_params(\n",
        "            head_dim=cfg[\"head_dim\"],\n",
        "            theta_base=cfg[\"rope_base\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            dtype=torch.float32,\n",
        "        )\n",
        "        # 将 RoPE 参数注册为 buffer，它们不是模型参数，但需要保存状态\n",
        "        self.register_buffer(\"cos_local\", cos_local, persistent=False)\n",
        "        self.register_buffer(\"sin_local\", sin_local, persistent=False)\n",
        "        self.register_buffer(\"cos_global\", cos_global, persistent=False)\n",
        "        self.register_buffer(\"sin_global\", sin_global, persistent=False)\n",
        "\n",
        "    # 创建注意力掩码\n",
        "    def _create_masks(self, seq_len, device):\n",
        "        # 创建一个全 1 的布尔张量\n",
        "        ones = torch.ones((seq_len, seq_len), dtype=torch.bool, device=device)\n",
        "\n",
        "        # mask_global (掩盖未来：j > i)\n",
        "        # 使用 triu 创建上三角矩阵，对角线偏移 1\n",
        "        mask_global = torch.triu(ones, diagonal=1)\n",
        "\n",
        "        # far_past (掩盖过远的过去：i - j >= sliding_window)\n",
        "        # 使用 triu 创建上三角矩阵，对角线偏移 sliding_window，然后转置\n",
        "        far_past = torch.triu(ones, diagonal=self.cfg[\"sliding_window\"]).T\n",
        "\n",
        "        # Local (sliding_window) = 未来 OR 过远的过去 (掩码)\n",
        "        # 将 mask_global 和 far_past 进行逻辑或运算\n",
        "        mask_local = mask_global | far_past\n",
        "        return mask_global, mask_local # 返回全局掩码和局部掩码\n",
        "\n",
        "    # 定义前向传播\n",
        "    def forward(self, input_ids):\n",
        "        # 获取批次大小和序列长度\n",
        "        b, seq_len = input_ids.shape\n",
        "        # 应用 token embedding，并进行缩放 (Gemma 使用特殊的缩放因子)\n",
        "        x = self.tok_emb(input_ids) * (self.cfg[\"emb_dim\"] ** 0.5)\n",
        "        # 创建注意力掩码\n",
        "        mask_global, mask_local = self._create_masks(seq_len, x.device)\n",
        "\n",
        "        # 遍历每个 Transformer 块并应用前向传播\n",
        "        for block in self.blocks:\n",
        "            x = block(\n",
        "                x,\n",
        "                mask_global=mask_global,\n",
        "                mask_local=mask_local,\n",
        "                cos_global=self.cos_global,\n",
        "                sin_global=self.sin_global,\n",
        "                cos_local=self.cos_local,\n",
        "                sin_local=self.sin_local,\n",
        "            )\n",
        "\n",
        "        # 应用最终 LayerNorm\n",
        "        x = self.final_norm(x)\n",
        "        # 应用输出头预测 logits，并转换到配置的数据类型\n",
        "        logits = self.out_head(x.to(self.cfg[\"dtype\"]))\n",
        "        return logits # 返回 logits"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be2d201f-74ad-4d63-ab9c-601b00674a48",
      "metadata": {
        "id": "be2d201f-74ad-4d63-ab9c-601b00674a48"
      },
      "source": [
        "&nbsp;\n",
        "# 2. Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa142fa-b375-4e78-b392-2072ced666f3",
      "metadata": {
        "id": "caa142fa-b375-4e78-b392-2072ced666f3"
      },
      "outputs": [],
      "source": [
        "# Gemma3 270M 模型的配置字典\n",
        "GEMMA3_CONFIG_270M = {\n",
        "    \"vocab_size\": 262_144, # 词汇表大小\n",
        "    \"context_length\": 32_768, # 上下文长度\n",
        "    \"emb_dim\": 640, # 嵌入维度\n",
        "    \"n_heads\": 4, # 注意力头的数量\n",
        "    \"n_layers\": 18, # Transformer 层的数量\n",
        "    \"hidden_dim\": 2048, # 前馈网络的隐藏层维度\n",
        "    \"head_dim\": 256, # 每个注意力头的维度\n",
        "    \"qk_norm\": True, # 是否进行 QK 归一化\n",
        "    \"n_kv_groups\": 1, # KV 组的数量 (1 表示多头注意力 MHA)\n",
        "    \"rope_local_base\": 10_000.0, # 局部 RoPE 的 theta_base\n",
        "    \"rope_base\": 1_000_000.0, # 全局 RoPE 的 theta_base\n",
        "    \"sliding_window\": 512, # 滑动窗口大小\n",
        "      \"layer_types\": [ # 每层的注意力类型\n",
        "        \"sliding_attention\", # 滑动窗口注意力\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\", # 全局注意力\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"sliding_attention\",\n",
        "        \"full_attention\"\n",
        "    ],\n",
        "    \"dtype\": torch.bfloat16, # 模型使用的数据类型\n",
        "    \"query_pre_attn_scalar\": 256, # 查询在注意力计算前的缩放因子\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "156253fe-aacd-4da2-8f13-705f05c4b11e",
      "metadata": {
        "id": "156253fe-aacd-4da2-8f13-705f05c4b11e"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123) # 设置随机种子以确保可复现性\n",
        "model = Gemma3Model(GEMMA3_CONFIG_270M) # 使用配置初始化 Gemma3 模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf86265-4e9d-4024-9ed0-99076944e304",
      "metadata": {
        "id": "eaf86265-4e9d-4024-9ed0-99076944e304"
      },
      "outputs": [],
      "source": [
        "model # 打印模型结构"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90aca91d-4bee-45ce-993a-4ec5393abe2b",
      "metadata": {
        "id": "90aca91d-4bee-45ce-993a-4ec5393abe2b"
      },
      "source": [
        "- A quick check that the forward pass works before continuing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adf0a6b7-b688-42c9-966e-c223d34db99f",
      "metadata": {
        "id": "adf0a6b7-b688-42c9-966e-c223d34db99f"
      },
      "outputs": [],
      "source": [
        "# 对输入张量进行一次前向传播，检查模型是否正常工作\n",
        "# torch.tensor([1, 2, 3]) 创建一个张量\n",
        "# .unsqueeze(0) 在第 0 维增加一个批次维度\n",
        "model(torch.tensor([1, 2, 3]).unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc",
      "metadata": {
        "id": "364e76ca-52f8-4fa5-af37-c4069f9694bc"
      },
      "outputs": [],
      "source": [
        "# 计算模型参数总数\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params:,}\") # 打印参数总数\n",
        "\n",
        "# 考虑权重共享，计算唯一的参数总数\n",
        "# 总参数减去 token embedding 层的参数数量（因为输出头与 token embedding 共享权重）\n",
        "total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\") # 打印唯一的参数总数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b",
      "metadata": {
        "id": "fd5efb03-5a07-46e8-8607-93ed47549d2b"
      },
      "outputs": [],
      "source": [
        "# 计算模型内存大小\n",
        "def model_memory_size(model, input_dtype=torch.float32):\n",
        "    total_params = 0 # 参数总数\n",
        "    total_grads = 0 # 梯度总数\n",
        "    # 遍历模型参数\n",
        "    for param in model.parameters():\n",
        "        # 计算每个参数的元素总数\n",
        "        param_size = param.numel()\n",
        "        total_params += param_size\n",
        "        # 检查参数是否需要计算梯度\n",
        "        if param.requires_grad:\n",
        "            total_grads += param_size\n",
        "\n",
        "    # 计算 buffer 的大小 (非参数但需要内存)\n",
        "    total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "    # 计算总内存大小 (字节) = (参数数量 + 梯度数量 + buffer 数量) * 元素大小\n",
        "    # 假设参数和梯度使用与输入 dtype 相同的数据类型存储\n",
        "    element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "    total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "    # 将字节转换为千兆字节 (GB)\n",
        "    total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "    return total_memory_gb # 返回内存大小 (GB)\n",
        "\n",
        "# 打印不同数据类型下的模型内存大小\n",
        "print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f12baf-f79b-499f-85c0-51328a6a20f5",
      "metadata": {
        "id": "31f12baf-f79b-499f-85c0-51328a6a20f5"
      },
      "outputs": [],
      "source": [
        "# 检查是否有可用的 CUDA 设备，否则检查 MPS 设备，最后使用 CPU\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\") # 使用 CUDA\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\") # 使用 MPS (Mac)\n",
        "else:\n",
        "    device = torch.device(\"cpu\") # 使用 CPU\n",
        "\n",
        "model.to(device); # 将模型移动到选定的设备上"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c172f89f-d301-439f-b809-46169e5f5945",
      "metadata": {
        "id": "c172f89f-d301-439f-b809-46169e5f5945"
      },
      "source": [
        "&nbsp;\n",
        "# 4. Load pretrained weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75166128-5899-4995-9b88-9672e135650e",
      "metadata": {
        "id": "75166128-5899-4995-9b88-9672e135650e"
      },
      "outputs": [],
      "source": [
        "# 将预训练权重加载到 Gemma 模型中\n",
        "def load_weights_into_gemma(Gemma3Model, param_config, params):\n",
        "\n",
        "    # 辅助函数：将源张量赋值给目标张量，并进行形状检查\n",
        "    def assign(left, right, tensor_name=\"unknown\"):\n",
        "        if left.shape != right.shape:\n",
        "            raise ValueError(\n",
        "                f\"Shape mismatch in tensor '{tensor_name}'. Left: {left.shape}, Right: {right.shape}\"\n",
        "            )\n",
        "        # 克隆并分离张量，然后转换为 nn.Parameter\n",
        "        return torch.nn.Parameter(right.clone().detach() if isinstance(right, torch.Tensor) else torch.tensor(right))\n",
        "\n",
        "    # 加载 Embedding 权重\n",
        "    if \"model.embed_tokens.weight\" in params:\n",
        "        model.tok_emb.weight = assign(\n",
        "            model.tok_emb.weight,\n",
        "            params[\"model.embed_tokens.weight\"],\n",
        "            \"model.embed_tokens.weight\",\n",
        "        )\n",
        "\n",
        "    # 遍历 Transformer 层并加载权重\n",
        "    for l in range(param_config[\"n_layers\"]):\n",
        "        block = model.blocks[l] # 获取当前 Transformer 块\n",
        "        att = block.att # 获取当前注意力层\n",
        "        # 注意力投影权重\n",
        "        att.W_query.weight = assign(\n",
        "            att.W_query.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.q_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.q_proj.weight\",\n",
        "        )\n",
        "        att.W_key.weight = assign(\n",
        "            att.W_key.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.k_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.k_proj.weight\",\n",
        "        )\n",
        "        att.W_value.weight = assign(\n",
        "            att.W_value.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.v_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.v_proj.weight\",\n",
        "        )\n",
        "        att.out_proj.weight = assign(\n",
        "            att.out_proj.weight,\n",
        "            params[f\"model.layers.{l}.self_attn.o_proj.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.o_proj.weight\",\n",
        "        )\n",
        "        # QK 归一化权重\n",
        "        att.q_norm.scale = assign(\n",
        "            att.q_norm.scale,\n",
        "            params[f\"model.layers.{l}.self_attn.q_norm.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.q_norm.weight\",\n",
        "        )\n",
        "        att.k_norm.scale = assign(\n",
        "            att.k_norm.scale,\n",
        "            params[f\"model.layers.{l}.self_attn.k_norm.weight\"],\n",
        "            f\"model.layers.{l}.self_attn.k_norm.weight\",\n",
        "        )\n",
        "        # 前馈网络权重\n",
        "        block.ff.fc1.weight = assign(\n",
        "            block.ff.fc1.weight,\n",
        "            params[f\"model.layers.{l}.mlp.gate_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.gate_proj.weight\",\n",
        "        )\n",
        "        block.ff.fc2.weight = assign(\n",
        "            block.ff.fc2.weight,\n",
        "            params[f\"model.layers.{l}.mlp.up_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.up_proj.weight\",\n",
        "        )\n",
        "        block.ff.fc3.weight = assign(\n",
        "            block.ff.fc3.weight,\n",
        "            params[f\"model.layers.{l}.mlp.down_proj.weight\"],\n",
        "            f\"model.layers.{l}.mlp.down_proj.weight\",\n",
        "        )\n",
        "        # LayerNorm 权重\n",
        "        block.input_layernorm.scale = assign(\n",
        "            block.input_layernorm.scale,\n",
        "            params[f\"model.layers.{l}.input_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.input_layernorm.weight\",\n",
        "        )\n",
        "        block.post_attention_layernorm.scale = assign(\n",
        "            block.post_attention_layernorm.scale,\n",
        "            params[f\"model.layers.{l}.post_attention_layernorm.weight\"],\n",
        "            f\"model.layers.{l}.post_attention_layernorm.weight\",\n",
        "        )\n",
        "        # 前馈网络前和后 LayerNorm 权重\n",
        "        pre_key = f\"model.layers.{l}.pre_feedforward_layernorm.weight\"\n",
        "        post_key = f\"model.layers.{l}.post_feedforward_layernorm.weight\"\n",
        "        if pre_key in params:\n",
        "            block.pre_feedforward_layernorm.scale = assign(\n",
        "                block.pre_feedforward_layernorm.scale,\n",
        "                params[pre_key],\n",
        "                pre_key,\n",
        "            )\n",
        "        if post_key in params:\n",
        "            block.post_feedforward_layernorm.scale = assign(\n",
        "                block.post_feedforward_layernorm.scale,\n",
        "                params[post_key],\n",
        "                post_key,\n",
        "            )\n",
        "\n",
        "    # 加载最终 LayerNorm 权重\n",
        "    if \"model.norm.weight\" in params:\n",
        "        model.final_norm.scale = assign(\n",
        "            model.final_norm.scale,\n",
        "            params[\"model.norm.weight\"],\n",
        "            \"model.norm.weight\",\n",
        "        )\n",
        "    # 加载输出头权重\n",
        "    if \"lm_head.weight\" in params:\n",
        "        model.out_head.weight = assign(\n",
        "            model.out_head.weight,\n",
        "            params[\"lm_head.weight\"],\n",
        "            \"lm_head.weight\",\n",
        "        )\n",
        "    elif \"model.embed_tokens.weight\" in params:\n",
        "        # 权重共享：重用 embedding 权重作为输出头权重\n",
        "        model.out_head.weight = assign(\n",
        "            model.out_head.weight,\n",
        "            params[\"model.embed_tokens.weight\"],\n",
        "            \"model.embed_tokens.weight\",\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "430340f2-78b9-4983-b74e-8395bbd7e574",
      "metadata": {
        "id": "430340f2-78b9-4983-b74e-8395bbd7e574"
      },
      "source": [
        "- Please note that Google requires that you accept the Gemma 3 licensing terms before you can download the files; to do this, you have to create a Hugging Face Hub account and visit the [google/gemma-3-270m]https://huggingface.co/google/gemma-3-270m) repository to accept the terms\n",
        "- Next, you will need to create an access token; to generate an access token with READ permissions, click on the profile picture in the upper right and click on \"Settings\"\n",
        "\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/settings.webp?1\" width=\"300px\">\n",
        "\n",
        "- Then, create and copy the access token so you can copy & paste it into the next code cell\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/bonus/gpt-to-llama/access-token.webp?1\" width=\"600px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cee5292-f756-41dd-9b8d-c9b5c25d23f8",
      "metadata": {
        "id": "7cee5292-f756-41dd-9b8d-c9b5c25d23f8"
      },
      "outputs": [],
      "source": [
        "# 如果是第一次运行 notebook，取消注释并运行以下代码进行 huggingface_hub 登录\n",
        "\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "WW2pZk6GK3ZY"
      },
      "id": "WW2pZk6GK3ZY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "8DjyRz2FK3ZZ"
      },
      "id": "8DjyRz2FK3ZZ"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "w9gbO2RUK3ZZ"
      },
      "id": "w9gbO2RUK3ZZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392",
      "metadata": {
        "id": "699cb1b8-a67d-49fb-80a6-0dad9d81f392"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from safetensors.torch import load_file # 从 safetensors 文件加载权重\n",
        "from huggingface_hub import hf_hub_download, snapshot_download # 从 Hugging Face Hub 下载文件\n",
        "from google.colab import userdata # 导入用于访问 Colab Secrets 的模块\n",
        "from huggingface_hub import login # 导入 login 函数\n",
        "\n",
        "# 从 Colab Secrets 获取 Hugging Face 令牌并登录\n",
        "try:\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if hf_token:\n",
        "        login(token=hf_token)\n",
        "        print(\"Successfully logged in to Hugging Face Hub.\")\n",
        "    else:\n",
        "        print(\"Hugging Face token not found in Colab Secrets. Please add it.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during Hugging Face login: {e}\")\n",
        "\n",
        "\n",
        "CHOOSE_MODEL = \"270m\" # 选择模型大小\n",
        "\n",
        "# 根据 USE_INSTRUCT_MODEL 标志确定 Hugging Face Hub 仓库 ID\n",
        "if USE_INSTRUCT_MODEL:\n",
        "    repo_id = f\"google/gemma-3-{CHOOSE_MODEL}-it\" # 指令模型仓库 ID\n",
        "else:\n",
        "    repo_id = f\"google/gemma-3-{CHOOSE_MODEL}\" # 基础模型仓库 ID\n",
        "\n",
        "\n",
        "local_dir = Path(repo_id).parts[-1] # 定义本地存储目录\n",
        "\n",
        "# 根据模型大小选择下载方式\n",
        "if CHOOSE_MODEL == \"270m\":\n",
        "    # 对于 270M 模型，直接下载 model.safetensors 文件\n",
        "    weights_file = hf_hub_download(\n",
        "        repo_id=repo_id, # 仓库 ID\n",
        "        filename=\"model.safetensors\", # 文件名\n",
        "        local_dir=local_dir, # 本地存储目录\n",
        "    )\n",
        "    weights_dict = load_file(weights_file) # 从 safetensors 文件加载权重到字典\n",
        "else:\n",
        "    # 对于其他模型大小 (如果存在分片)，下载整个仓库快照\n",
        "    repo_dir = snapshot_download(repo_id=repo_id, local_dir=local_dir)\n",
        "    index_path = os.path.join(repo_dir, \"model.safetensors.index.json\") # 权重索引文件路径\n",
        "    with open(index_path, \"r\") as f:\n",
        "        index = json.load(f) # 加载权重索引\n",
        "\n",
        "    weights_dict = {} # 初始化权重字典\n",
        "    # 遍历索引中的文件名并加载权重\n",
        "    for filename in set(index[\"weight_map\"].values()):\n",
        "        shard_path = os.path.join(repo_dir, filename) # 分片文件路径\n",
        "        shard = load_file(shard_path) # 加载分片权重\n",
        "        weights_dict.update(shard) # 更新权重字典\n",
        "\n",
        "# 将加载的权重加载到模型中\n",
        "load_weights_into_gemma(model, GEMMA3_CONFIG_270M, weights_dict)\n",
        "model.to(device) # 将模型移动到设备\n",
        "del weights_dict # 删除权重字典以释放内存"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b345491-3510-4397-92d3-cd0a3fa3deee",
      "metadata": {
        "id": "6b345491-3510-4397-92d3-cd0a3fa3deee"
      },
      "source": [
        "&nbsp;\n",
        "# 4. Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b68ab489-48e5-471e-a814-56cda2d60f81",
      "metadata": {
        "id": "b68ab489-48e5-471e-a814-56cda2d60f81"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer # 导入 Tokenizer 类\n",
        "\n",
        "# 定义 Gemma 分词器类\n",
        "class GemmaTokenizer:\n",
        "    def __init__(self, tokenizer_file_path: str):\n",
        "        tok_file = Path(tokenizer_file_path) # 创建 Path 对象\n",
        "        self._tok = Tokenizer.from_file(str(tok_file)) # 从文件加载分词器\n",
        "        # 尝试识别 EOS 和 padding token\n",
        "        eos_token = \"<end_of_turn>\" # 定义 EOS token 字符串\n",
        "        self.pad_token_id = eos_token # 设置 padding token ID\n",
        "        self.eos_token_id = eos_token # 设置 EOS token ID\n",
        "\n",
        "    # 将文本编码为 token ID 列表\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        return self._tok.encode(text).ids\n",
        "\n",
        "    # 将 token ID 列表解码为文本\n",
        "    def decode(self, ids: list[int]) -> str:\n",
        "        return self._tok.decode(ids, skip_special_tokens=False) # 不跳过特殊 token\n",
        "\n",
        "# 应用聊天模板，格式化用户输入\n",
        "def apply_chat_template(user_text):\n",
        "    return f\"<start_of_turn>user\\n{user_text}<end_of_turn>\\n<start_of_turn>model\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b6df8bc-7308-468e-93ce-2d5529ea7866",
      "metadata": {
        "id": "7b6df8bc-7308-468e-93ce-2d5529ea7866"
      },
      "outputs": [],
      "source": [
        "tokenizer_file_path = os.path.join(local_dir, \"tokenizer.json\") # 构建 tokenizer 文件路径\n",
        "# 如果文件不存在，则尝试从 Hugging Face Hub 下载\n",
        "if not os.path.exists(tokenizer_file_path):\n",
        "    try:\n",
        "        tokenizer_file_path = hf_hub_download(repo_id=repo_id, filename=\"tokenizer.json\", local_dir=local_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: failed to download tokenizer.json: {e}\")\n",
        "        tokenizer_file_path = \"tokenizer.json\" # 如果下载失败，使用默认文件名\n",
        "\n",
        "tokenizer = GemmaTokenizer(tokenizer_file_path=tokenizer_file_path) # 初始化 Gemma 分词器"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1946b534-e3af-431a-a222-391a60bfa892",
      "metadata": {
        "id": "1946b534-e3af-431a-a222-391a60bfa892"
      },
      "outputs": [],
      "source": [
        "prompt = \"深入浅出地用简体中文解释一下统计学中的三门问题\" # 定义原始 prompt\n",
        "# 应用聊天模板格式化 prompt\n",
        "prompt = apply_chat_template(\"深入浅出地用简体中文解释一下统计学中的三门问题\")\n",
        "\n",
        "\n",
        "input_token_ids = tokenizer.encode(prompt) # 将 prompt 编码为 token ID 列表\n",
        "text = tokenizer.decode(input_token_ids) # 将 token ID 列表解码回文本\n",
        "text # 打印解码后的文本 (包含特殊 token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57d07df1-4401-4792-b549-7c4cc5632323",
      "metadata": {
        "id": "57d07df1-4401-4792-b549-7c4cc5632323"
      },
      "source": [
        "&nbsp;\n",
        "# 5. Generate text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5",
      "metadata": {
        "id": "7b8401c6-e244-4cb7-9849-2ba71ce758d5"
      },
      "outputs": [],
      "source": [
        "# 定义基本的文本生成器 (流式输出)\n",
        "def generate_text_basic_stream(model, token_ids, max_new_tokens, eos_token_id=None):\n",
        "\n",
        "    model.eval() # 将模型设置为评估模式\n",
        "    with torch.no_grad(): # 禁用梯度计算\n",
        "        # 循环生成 max_new_tokens 个 token\n",
        "        for _ in range(max_new_tokens):\n",
        "            # 前向传播，获取最后一个 token 位置的 logits\n",
        "            out = model(token_ids)[:, -1]\n",
        "            # 找到 logits 中概率最大的 token 作为下一个 token\n",
        "            next_token = torch.argmax(out, dim=-1, keepdim=True)\n",
        "\n",
        "            # 如果生成了 EOS token，则停止生成\n",
        "            if (eos_token_id is not None\n",
        "                   and torch.all(next_token == eos_token_id)):\n",
        "               break\n",
        "\n",
        "            yield next_token # 生成下一个 token\n",
        "\n",
        "            # 将新生成的 token 添加到 token_ids 中\n",
        "            token_ids = torch.cat([token_ids, next_token], dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d",
      "metadata": {
        "id": "1c7a04fa-6aac-416b-8f63-f1e19227633d"
      },
      "outputs": [],
      "source": [
        "# 将输入 token ID 列表转换为张量，并增加批次维度，移动到设备上\n",
        "input_token_ids_tensor = torch.tensor(input_token_ids, device=device).unsqueeze(0)\n",
        "\n",
        "# 使用流式生成器生成文本并打印\n",
        "for token in generate_text_basic_stream(\n",
        "    model=model, # 模型\n",
        "    token_ids=input_token_ids_tensor, # 输入 token ID 张量\n",
        "    max_new_tokens=500, # 最大生成 token 数量\n",
        "    eos_token_id=tokenizer.encode(\"<end_of_turn>\")[-1] # EOS token ID\n",
        "):\n",
        "    token_id = token.squeeze(0).tolist() # 从张量中提取 token ID 并转换为列表\n",
        "    # 解码 token ID 并打印，不换行，并刷新输出缓冲区\n",
        "    print(\n",
        "        tokenizer.decode(token_id),\n",
        "        end=\"\",\n",
        "        flush=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "549324d6-5c71-4147-ae21-2e67675faa3d",
      "metadata": {
        "id": "549324d6-5c71-4147-ae21-2e67675faa3d"
      },
      "source": [
        "&nbsp;\n",
        "# What's next?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c",
      "metadata": {
        "id": "e6edaaae-2de1-406c-8ffa-897cdfa3808c"
      },
      "source": [
        "- Check out the [README.md](./README.md), to use this model via the `llms_from_scratch` package\n",
        "- For those interested in a comprehensive guide on building a large language model from scratch and gaining a deeper understanding of its mechanics, you might like my [Build a Large Language Model (From Scratch)](http://mng.bz/orYv)\n",
        "\n",
        "<a href=\"http://mng.bz/orYv\"><img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/cover-small.webp\" width=\"100px\"></a>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}